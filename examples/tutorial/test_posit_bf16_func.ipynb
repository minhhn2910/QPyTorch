{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f62f72cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/himeshi/.cache/torch_extensions/py310_cu126 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/himeshi/.cache/torch_extensions/py310_cu126/quant_cpu/build.ninja...\n",
      "Building extension module quant_cpu...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module quant_cpu...\n",
      "Using /home/himeshi/.cache/torch_extensions/py310_cu126 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/himeshi/.cache/torch_extensions/py310_cu126/quant_cuda/build.ninja...\n",
      "/home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/utils/cpp_extension.py:2356: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module quant_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF quant_cuda.o.d -DTORCH_EXTENSION_NAME=quant_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/include -isystem /home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/himeshi/.pyenv/versions/3.10.4/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -fPIC -std=c++17 -O3 -std=c++17 -fPIC -c /home/himeshi/conga25/QPyTorch/qtorch/quant/quant_cuda/quant_cuda.cpp -o quant_cuda.o \n",
      "[2/3] /usr/local/cuda/bin/nvcc --generate-dependencies-with-compile --dependency-output quant.cuda.o.d -DTORCH_EXTENSION_NAME=quant_cuda -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1016\\\" -isystem /home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/include -isystem /home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -isystem /usr/local/cuda/include -isystem /home/himeshi/.pyenv/versions/3.10.4/include/python3.10 -D_GLIBCXX_USE_CXX11_ABI=1 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_86,code=compute_86 -gencode=arch=compute_86,code=sm_86 --compiler-options '-fPIC' -O3 --use_fast_math -std=c++17 -c /home/himeshi/conga25/QPyTorch/qtorch/quant/quant_cuda/quant.cu -o quant.cuda.o \n",
      "[3/3] c++ quant_cuda.o bit_helper.cuda.o sim_helper.cuda.o block_kernel.cuda.o float_kernel.cuda.o fixed_point_kernel.cuda.o quant.cuda.o posit_kernel.cuda.o -shared -shared -L/home/himeshi/conga25/conga25env/lib/python3.10/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda/lib64 -lcudart -o quant_cuda.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module quant_cuda...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import qtorch\n",
    "from qtorch.quant import bfloat16_posit8_quantize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f0e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-20.0000, -17.5000, -15.0000, -12.5000, -10.0000,  -7.5000,  -5.0000,\n",
      "         -2.5000,   0.0000,   2.5000,   5.0000,   7.5000,  10.0000,  12.5000,\n",
      "         15.0000,  17.5000], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(-20,20,2.5)\n",
    "a = torch.tensor(a, dtype=torch.bfloat16)\n",
    "print (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96fc5600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "tensor([-20.0000, -16.0000, -15.0000, -12.0000, -10.0000,  -7.5000,  -5.0000,\n",
      "         -2.5000,   0.0000,   2.5000,   5.0000,   7.5000,  10.0000,  12.0000,\n",
      "         15.0000,  16.0000], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "p = bfloat16_posit8_quantize(a,nsize=8,es=1)\n",
    "print(p.dtype)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7df29982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "tensor([-16.0000, -16.0000, -16.0000,  -8.0000,  -6.0000,  -4.0000,  -6.0000,\n",
      "         -1.5000,   0.0000,   1.5000,   6.0000,   4.0000,   6.0000,   8.0000,\n",
      "         16.0000,  16.0000], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "p2= bfloat16_posit8_quantize(a,nsize=7,es=1)\n",
    "print(p2.dtype)\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "678091aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.bfloat16\n",
      "tensor([-16., -16., -16.,  -4.,  -4.,  -4.,  -4.,  -2.,   0.,   2.,   4.,   4.,\n",
      "          4.,   4.,  16.,  16.], dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "p3 = bfloat16_posit8_quantize(a,nsize=6,es=1)\n",
    "print(p3.dtype)\n",
    "print(p3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conga25env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
